<!DOCTYPE html>
<!-- saved from url=(0087)https://elvinouyang.github.io/study%20notes/python-linear-models-for-continuous-target/ -->
<html lang="en" class=" js "><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- begin SEO -->









<title>            Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target      Elvin Ouyang’s Blog      </title>




<meta name="description" content="```pythonimport sysprint(“Python version: {}”.format(sys.version))import pandas as pdprint(“pandas version: {}”.format(pd.version))import matplotlibprint(“matplotlib version: {}”.format(matplotlib.version))import numpy as npprint(“NumPy version: {}”.format(np.version))import scipy as spprint(“SciPy version: {}”.format(sp.version))import IPythonprint(“IPython version: {}”.format(IPython.version))import sklearnprint(“scikit-learn version: {}”.format(sklearn.version))import mglearnprint(“mglearn version: {}”.format(mglearn.version))import mathimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split">




<meta name="author" content="Chuanye (Elvin) Ouyang">

<meta property="og:locale" content="en">
<meta property="og:site_name" content="Elvin Ouyang&#39;s Blog">
<meta property="og:title" content="Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target">


  <link rel="canonical" href="https://elvinouyang.github.io/study%20notes/python-linear-models-for-continuous-target/">
  <meta property="og:url" content="https://elvinouyang.github.io/study%20notes/python-linear-models-for-continuous-target/">



  <meta property="og:description" content="```pythonimport sysprint(“Python version: {}”.format(sys.version))import pandas as pdprint(“pandas version: {}”.format(pd.version))import matplotlibprint(“matplotlib version: {}”.format(matplotlib.version))import numpy as npprint(“NumPy version: {}”.format(np.version))import scipy as spprint(“SciPy version: {}”.format(sp.version))import IPythonprint(“IPython version: {}”.format(IPython.version))import sklearnprint(“scikit-learn version: {}”.format(sklearn.version))import mglearnprint(“mglearn version: {}”.format(mglearn.version))import mathimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2017-01-21T00:00:00-05:00">








  <script async="" src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/analytics.js.download"></script><script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Elvin Ouyang",
      "url" : "https://elvinouyang.github.io",
      "sameAs" : ["https://twitter.com/elvinouyang","https://www.linkedin.com/in/ouyangchuanye/"]
    }
  </script>






<!-- end SEO -->


<link href="https://elvinouyang.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Elvin Ouyang&#39;s Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->

<meta http-equiv="cleartype" content="on">
    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  <script type="text/javascript" async="" src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/embed.js.download"></script><script async="" type="text/javascript" src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/count.js.download"></script><style id="fit-vids-style">.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style><link rel="preload" as="style" href="https://c.disquscdn.com/next/embed/styles/lounge.d49f53e192b9080ef8880a7c9b24f1c3.css"><link rel="preload" as="script" href="https://c.disquscdn.com/next/embed/common.bundle.c1c826ba467260790d5c05dc3cc453f8.js"><link rel="preload" as="script" href="https://c.disquscdn.com/next/embed/lounge.bundle.c8f0dc192ba789fde18d281769beb36d.js"><link rel="preload" as="script" href="https://disqus.com/next/config.js"><script src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/alfalfa_min.d078e4f2a4721192a99e02601a767617.js.download" async="" charset="UTF-8"></script></head>

  <body class="layout--single" style="margin-bottom: 221px;">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://elvinouyang.github.io/">Elvin Ouyang's Blog</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item"><a href="https://elvinouyang.github.io/about/">About</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://elvinouyang.github.io/projects/">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://elvinouyang.github.io/study-notes/">Study Notes</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://elvinouyang.github.io/categories/">Category</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://elvinouyang.github.io/tags/">Tag</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://elvinouyang.github.io/resources/">Resources</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://elvinouyang.github.io/sitemap/">Sitemap</a></li>
          
        </ul>
        <button count="0" class="hidden"><div class="navicon"></div></button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  
  <div class="sidebar sticky">
  

<div itemscope="" itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/bio-photo.jpeg" class="author__avatar" alt="Chuanye (Elvin) Ouyang" itemprop="image">
      
    </div>
  

  <div class="author__content">
    <h3 class="author__name" itemprop="name">Chuanye (Elvin) Ouyang</h3>
    
      <p class="author__bio" itemprop="description">
        Chuanye (Elvin) Ouyang is a young professional in data science, risk analysis, compliance evaluation, and forensic analytics.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope="" itemtype="http://schema.org/Place">
          <i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> <span itemprop="name">Washington, DC</span>
        </li>
      

      

      

      

      
        <li>
          <a href="https://twitter.com/elvinouyang" itemprop="sameAs">
            <i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/ouyangchuanye" itemprop="sameAs">
            <i class="fa fa-fw fa-linkedin-square" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/ElvinOuyang" itemprop="sameAs">
            <i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fa fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope="" itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target">
    <meta itemprop="description" content="```pythonimport sysprint(“Python version: {}”.format(sys.version))import pandas as pdprint(“pandas version: {}”.format(pd.version))import matplotlibprint(“matplotlib version: {}”.format(matplotlib.version))import numpy as npprint(“NumPy version: {}”.format(np.version))import scipy as spprint(“SciPy version: {}”.format(sp.version))import IPythonprint(“IPython version: {}”.format(IPython.version))import sklearnprint(“scikit-learn version: {}”.format(sklearn.version))import mglearnprint(“mglearn version: {}”.format(mglearn.version))import mathimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split">
    <meta itemprop="datePublished" content="January 21, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 




  8 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Python version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="k">print</span><span class="p">(</span><span class="s">"pandas version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="k">print</span><span class="p">(</span><span class="s">"matplotlib version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">print</span><span class="p">(</span><span class="s">"NumPy version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="n">sp</span>
<span class="k">print</span><span class="p">(</span><span class="s">"SciPy version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sp</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="k">print</span><span class="p">(</span><span class="s">"IPython version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="k">print</span><span class="p">(</span><span class="s">"scikit-learn version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="k">print</span><span class="p">(</span><span class="s">"mglearn version: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mglearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Python version: 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
pandas version: 0.19.2
matplotlib version: 1.5.3
NumPy version: 1.11.3
SciPy version: 0.18.1
IPython version: 5.1.0
scikit-learn version: 0.18.1
mglearn version: 0.1.3
</code></pre></div></div>

<h2 id="15-linear-models-for-regression">1.5 Linear Models for Regression</h2>

<blockquote>
  <p>For regression, the general prediction formula for a linear model looks as follows:</p>
</blockquote>

<blockquote>
  <p><em>ŷ = w[0] * x[0] + w[1] * x[1] + … + w[p] * x[p] + b</em></p>
</blockquote>

<blockquote>
  <p>Here, x[0] to x[p] denotes the features (in this example, the number of features is p)
of a single data point, w and b are parameters of the model that are learned, and ŷ is
the prediction the model makes. For a dataset with a single feature, this is:</p>
</blockquote>

<blockquote>
  <p><em>ŷ = w[0] * x[0] + b</em></p>
</blockquote>

<p>For Linear Regression models, the bigger the absolute values of the coefficients, the more complex the model is. In other words, the flatter the linear line, the simpler the model is.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_linear_regression_wave</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w[0]: 0.393906  b: -0.031804
</code></pre></div></div>

<p><img src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/Introduction to Machine Learning with Python - Chapter 2 - Linear.png" alt="png"></p>

<p><strong>Definition of Linear Models</strong></p>
<blockquote>
  <p>Linear models for regression can be characterized as regression models for which the
prediction is a line for a single feature, a plane when using two features, or a hyperplane
in higher dimensions (that is, when using more features).</p>
</blockquote>

<p>Linear models can be extra powerful for a <em>wide dataset</em>, i.e. dataset with more features than training data points. We can start by learning the most popular regression model.</p>

<h3 id="151-lenear-regression-aka-ordinary-least-squares-ols">1.5.1 Lenear regression (aka ordinary least squares, OLS)</h3>

<blockquote>
  <p>Linear regression finds the parameters w and b that <em>minimize
the mean squared error</em> between predictions and the true regression targets, y, on the training set.</p>
</blockquote>

<p><strong>Features of OLS</strong></p>

<ul>
  <li>No parameters</li>
  <li>Cannot control model complexity</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_wave</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">60</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p>The fitted OLS model has two parameters, as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"lr.coef_: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"lr.intercept_: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>

<span class="c"># parameters generated from the training data will have "_" at the</span>
<span class="c"># tail, whereas parameters set by users don't</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lr.coef_: [ 0.39390555]
lr.intercept_: -0.03180434302675973
</code></pre></div></div>

<p>The model’s performance on training set and test set is as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set score: 0.67
Test set score: 0.66
</code></pre></div></div>

<p>The linear regression’s performance on a simple dataset is not really impressive. But we will now test it on a complex <strong>Boston Housing</strong> dataset.</p>

<h3 id="1511-ols-on-boston-housing-dataset">1.5.1.1 OLS on Boston Housing dataset</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_extended_boston</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set score: 0.95
Test set score: 0.61
</code></pre></div></div>

<p>Since OLS does not give options to <strong>control complexity</strong>, this model is overfitting. We will then look at an alternative to OLS, which is <strong>Ridge Regression</strong>.</p>

<h3 id="152-ridge-regression">1.5.2 Ridge Regression</h3>

<p><strong>Ridge Regression</strong> is the basic OLS model plus the following restriction:</p>

<blockquote>
  <p>We also want <strong><em>the magnitude of coefficients
to be as small as possible</em></strong>; in other words, all entries of w should be close to
zero. Intuitively, this means each feature should have as little effect on the outcome as
possible (which translates to having a small slope), while still predicting well.</p>
</blockquote>

<p>This process, called <strong><em>L2 regularization</em></strong> helps to explicitly avoid overfitting.</p>

<p>The sklearn uses <strong>Ridge</strong> class to instantiate Ridge Regressions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set score: 0.89
Test set score: 0.75
</code></pre></div></div>

<p>The <em>default</em> parameter for Ridge model is alpha = 1.0. By changing the alpha value when instantiating the class, users can adjust the level of restrictions on the Ridge Regression.</p>

<p><strong>Higher Alpha</strong> -&gt; <strong>Stronger restriction</strong> -&gt; <strong>Parameter magnitude closer to zero</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ridge10</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge10</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge10</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="n">ridge01</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set score: 0.79
Test set score: 0.64

Training set score: 0.93
Test set score: 0.77
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_extended_boston</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">training_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c"># try alpha from 0.01 to 10</span>
<span class="n">alpha_settings</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_settings</span><span class="p">:</span>
    <span class="c"># build the model</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># record training accuracy</span>
    <span class="n">training_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="c"># record generalization accuracy</span>
    <span class="n">test_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)),</span>
         <span class="n">training_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"training accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)),</span>
        <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"test accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"alpha logs"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7fd00d0c76d8&gt;
</code></pre></div></div>

<p><img src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/Introduction to Machine Learning with Python - Chapter 2 - (1).png" alt="png"></p>

<p>As we can see from above graph, the performance of Ridge Regression model on the test set hits the highest point when the natural logarithm of the alpha is between -2 and -1. Considering the decreasing nature of the training accuracy, we should prefer an alpha within this range that is as low as possible.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"log alpha array: {}"</span><span class="o">.</span>
      <span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)),</span><span class="mi">3</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"alpha array: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training performance: {:.5f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test performance: {:.5f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log alpha array: [-4.605 -2.996 -2.303 -1.609 -1.204 -0.916 -0.693 -0.511 -0.357 -0.223
 -0.105  0.     0.693  1.099  1.386  1.609  1.792  1.946  2.079  2.197
  2.303]
alpha array: [  0.01   0.05   0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9
   1.     2.     3.     4.     5.     6.     7.     8.     9.    10.  ]
Training performance: 0.92028
Test performance: 0.77468
</code></pre></div></div>

<p>From the alpha arrays, we observe that the fourth alpha meets our conditions; i.e. <em>when alpha = 0.2</em>, we will see the optimized Ridge Model.</p>

<h3 id="1521-under-the-hood-for-ridge-regressions">1.5.2.1 Under the Hood for Ridge Regressions</h3>

<blockquote>
  <p>We can also get a more qualitative insight into how the alpha parameter changes the
model by inspecting the <strong>coef_ attribute of models with different values of alpha</strong>. A
higher alpha means a more restricted model, so we expect the entries of coef_ to
have smaller magnitude for a high value of alpha than for a low value of alpha.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'s'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Ridge alpha=1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge10</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'^'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Ridge alpha=10"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'v'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Ridge alpha=0.1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"LinearRegression"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Coefficient index"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Coefficient magnitude"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7fd00d019eb8&gt;
</code></pre></div></div>

<p><img src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/Introduction to Machine Learning with Python - Chapter 2 - (2).png" alt="png"></p>

<p>From above graph we can see that with alpha larger, the coefficients are most condensed near the <em>w=0</em> line.</p>

<p><strong>Learning Curves</strong></p>
<blockquote>
  <p>Another way to understand the influence of regularization is to fix a value of alpha
but vary the amount of training data available.</p>
</blockquote>

<blockquote>
  <p>we subsampled the
Boston Housing dataset and evaluated LinearRegression and Ridge(alpha=1) on
subsets of increasing size (<strong>plots that show model performance as a function of dataset
size are called learning curves</strong>)</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_ridge_n_samples</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/Introduction to Machine Learning with Python - Chapter 2 - (3).png" alt="png"></p>

<blockquote>
  <p>Because ridge is regularized, <em>the training
score of ridge is lower than the training score</em> for linear regression across the board.
However, <strong>the test score for ridge is better, particularly for small subsets of the data</strong>.
For less than 400 data points, linear regression is not able to learn anything. As more
and more data becomes available to the model, both models improve, and linear
regression catches up with ridge in the end.</p>
</blockquote>

<p><strong>Key Takeaway</strong></p>

<p>With enough training data, regularization becomes less important, and given enough data, ridge and linear regression will have the same performance.</p>

<h3 id="153-lasso-regression">1.5.3 Lasso Regression</h3>

<p><strong>Lasso Regression</strong> uses <em>L1 Regualtion</em> to regularize the regression.</p>

<blockquote>
  <p>The consequence of L1 regularization
is that when using the lasso, some coefficients are <strong>exactly zero</strong>. This means some features
are entirely ignored by the model. This can be seen as <strong>a form of automatic feature
selection</strong>. Having some coefficients be exactly zero often makes a model easier to
interpret, and can <strong>reveal the most important features of your model</strong>.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Number of features used: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set score: 0.29
Test set score: 0.21
Number of features used: 4
</code></pre></div></div>

<p>Apparently the default setting for this <em>lasso regression</em> is underfitting on the Houston Housing dataset.</p>

<p>Similarly, <strong>Lasso Regression</strong> also has <em>alpha = 1.0</em> as its parameter. The larger alpha is, the simpler the model is.</p>

<p>Another parameter, <em>max_iter</em> (maximum number of iterations to run) should also be defined. The smaller alpha is, the larger max_iter should be.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># We increase the default setting of max_iter</span>
<span class="c"># And decrease alpha</span>
<span class="n">lasso001</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Number of features used: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set score: 0.90
Test set score: 0.77
Number of features used: 33
</code></pre></div></div>

<p>The adjusted model ended up using 33 out of the 105 features in the dataset. This makes it potentially easier to understand.</p>

<p>If we set <em>alpha</em> too low, the model will become too complicated and more similar to OLS.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lasso00001</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test set score: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Number of features used: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set score: 0.95
Test set score: 0.64
Number of features used: 94
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">training_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c"># try alpha from 0.00001 to 10</span>
<span class="n">alpha_settings</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.00001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alpha_settings</span><span class="p">:</span>
    <span class="c"># build the model</span>
    <span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000000</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># record training accuracy</span>
    <span class="n">training_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="c"># record generalization accuracy</span>
    <span class="n">test_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)),</span>
         <span class="n">training_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"training accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)),</span>
        <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"test accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Accuracy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"alpha logs"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x7fd00cf92860&gt;
</code></pre></div></div>

<p><img src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/Introduction to Machine Learning with Python - Chapter 2 - (4).png" alt="png"></p>

<p>Similarly to the <em>redge regression</em>, the “sweet spot” for this specific case comes at -6 &lt; log(alpha) &lt; -4. We can check it out by looking at the corresponding arrays.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"log alpha array: {}"</span><span class="o">.</span>
      <span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)),</span><span class="mi">3</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"alpha array: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Sweet spot alpha: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha_settings</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training performance: {:.5f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">training_accuracy</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test performance: {:.5f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log alpha array: [-11.513  -9.21   -6.908  -4.605  -2.996  -2.303  -0.693   0.      0.693
   1.386   2.079   2.303]
alpha array: [  1.00000000e-05   1.00000000e-04   1.00000000e-03   1.00000000e-02
   5.00000000e-02   1.00000000e-01   5.00000000e-01   1.00000000e+00
   2.00000000e+00   4.00000000e+00   8.00000000e+00   1.00000000e+01]
Sweet spot alpha: 0.01
Training performance: 0.89651
Test performance: 0.76565
</code></pre></div></div>

<h3 id="1531-under-the-hood-for-lasso-regressions">1.5.3.1 Under the Hood for Lasso Regressions</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'s'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Lasso alpha=1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'^'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Lasso alpha=0.01"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'v'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Lasso alpha=0.0001"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Ridge alpha=0.1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Coefficient index"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Coefficient magnitude"</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.text.Text at 0x7fd00ce0b2e8&gt;
</code></pre></div></div>

<p><img src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/Introduction to Machine Learning with Python - Chapter 2 - (5).png" alt="png"></p>

<blockquote>
  <p>In practice, <strong>ridge regression</strong> is usually the <strong>first</strong> choice between these two models.</p>
</blockquote>

<blockquote>
  <p>However, if you have <strong>a large amount of features and expect only a few of them to be
important, Lasso</strong> might be a better choice. Similarly, if you would like to have a
model that is <strong>easy to interpret, Lasso will provide a model that is easier to understand</strong>,
as it will select only a subset of the input features.</p>
</blockquote>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://elvinouyang.github.io/tags/#machine-learning" class="page__taxonomy-item" rel="tag">machine-learning</a><span class="sep">, </span>
    
      
      
      <a href="https://elvinouyang.github.io/tags/#python" class="page__taxonomy-item" rel="tag">Python</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://elvinouyang.github.io/categories/#study-notes" class="page__taxonomy-item" rel="tag">Study Notes</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-01-21T00:00:00-05:00">January 21, 2017</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="https://elvinouyang.github.io/study%20notes/python-datasets-and-knn/" class="pagination--pager" title="Introduction to Machine Learning with Python - Chapter 2 - Datasets and kNN
">Previous</a>
    
    
      <a href="https://elvinouyang.github.io/study%20notes/python-linear-models-for-classification/" class="pagination--pager" title="Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Classification
">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
    <h4 class="page__comments-title">Leave a Comment</h4>
    <section id="disqus_thread"><iframe id="dsq-app1343" name="dsq-app1343" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/saved_resource.html" style="width: 1px !important; min-width: 100% !important; border: none !important; overflow: hidden !important; height: 391px !important;" horizontalscrolling="no" verticalscrolling="no"></iframe></section>
  
</div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/ElvinOuyang"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://elvinouyang.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2017 Elvin Ouyang. Powered by <a href="http://jekyllrb.com/" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/main.min.js.download"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-96112090-1', 'auto');
  ga('send', 'pageview');
</script>







  
  <script type="text/javascript">
  	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  	var disqus_shortname = 'elvinouyang-github-io';

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function() {
  		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  	})();

  	/* * * DON'T EDIT BELOW THIS LINE * * */
  	(function () {
  		var s = document.createElement('script'); s.async = true;
  		s.type = 'text/javascript';
  		s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  		(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  	}());
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






  

<iframe style="display: none;" src="./Introduction to Machine Learning with Python - Chapter 2 - Linear Models for Continuous Target Elvin Ouyang’s Blog_files/saved_resource(1).html"></iframe></body></html>